{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTIMENT IDENTIFICATION\n",
    "\n",
    "1. Human Ratings Task:\n",
    "a) Get 3 classmates (opinion holders) to write three different opinions about their phone.\n",
    "b) Get 3 different people (raters) to rate these comments as positive, negative, neutral or can’t-say\n",
    "c) Take this 3 x 3 matrix and find the inter-rater reliability between your 3 raters using Kappa\n",
    "d) If you wanted to get the correlation between raters (using Pearson’s rho) what would you do? Then do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opinions:\n",
    "#1. It's expensive and the battery doesn't last as long as I'd like, but that hasn't stopped me from thoroughly\n",
    "    #enjoying my time with the Pixel 4 XL. This is a phone where the overall experience is greater than the sum of its parts, and at the end of the day, it's the Android handset I most eagerly reach for.\n",
    "#2. Awful. After less than six months the screen is not working. It not original I guess.\n",
    "#3. I was very excited when i received my phone. Fast processing, however after about two months of using, \n",
    "    #white lines appeared on the screen which then cost me additional money to replace the screens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rating are as follows: \n",
    "    # + -> +1\n",
    "    # - -> 0\n",
    "    # N -> -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-Rater Reliability between Rater1 and Rater2:  0.4999999999999999\n",
      "Inter-Rater Reliability between Rater1 and Rater3:  1.0\n",
      "Inter-Rater Reliability between Rater2 and Rater3:  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "rater1 = [1,0,-1]\n",
    "rater2 = [-1,0,-1]\n",
    "rater3 = [1,0,-1]\n",
    "\n",
    "print('Inter-Rater Reliability between Rater1 and Rater2: ',cohen_kappa_score(rater1, rater2))\n",
    "print('Inter-Rater Reliability between Rater1 and Rater3: ',cohen_kappa_score(rater1, rater3))\n",
    "print('Inter-Rater Reliability between Rater2 and Rater3: ',cohen_kappa_score(rater2, rater3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pearson_correlation(numbers_x, numbers_y):\n",
    "    x = numbers_x - numbers_x.mean()\n",
    "    y = numbers_y - numbers_y.mean()\n",
    "    return (x * y).sum() / np.sqrt((x**2).sum() * (y**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=np.array(rater1)\n",
    "r2=np.array(rater2)\n",
    "r3=np.array(rater3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  0 -1] [-1  0 -1] [ 1  0 -1]\n"
     ]
    }
   ],
   "source": [
    "print(r1,r2,r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's rho between Rater1 and Rater2 0.0\n",
      "Pearson's rho between Rater1 and Rater3 1.0\n",
      "Pearson's rho between Rater2 and Rater3 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Pearson\\'s rho between Rater1 and Rater2',pearson_correlation(r1,r2))\n",
    "print('Pearson\\'s rho between Rater1 and Rater3',pearson_correlation(r1,r3))\n",
    "print('Pearson\\'s rho between Rater2 and Rater3',pearson_correlation(r2,r3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's rho between Rater2 and Rater3 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Pearson\\'s rho between Rater2 and Rater3',pearson_correlation(r2,r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Bromberg’s Sentiment Program:\n",
    "Have a look at the simple program that does sentiment analysis. So, take a look at the program and see what is\n",
    "happening in the different variables, but adding print statements on its variables.\n",
    "a) Now consider ways to improve the training. Eg if you removed stopwords from the inputs what do you think might\n",
    "happen?\n",
    "b) Implement this or another solution in the program and report what happens to the precision and recall of the\n",
    "classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.77344336084021\n",
      "pos precision: 0.7881422924901186\n",
      "pos recall: 0.7479369842460615\n",
      "neg precision: 0.7601713062098501\n",
      "neg recall: 0.7989497374343586\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     17.0 : 1.0\n",
      "                   quiet = True              pos : neg    =     15.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                portrait = True              pos : neg    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "                   flaws = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# http://andybromberg.com/sentiment-analysis-python/\n",
    "# Andy Bromberg's Simple Sentiment Analysis System\n",
    "# Uses data from Pang & Lee (2005)\n",
    "# Uses a Naive Bayes Classifier Train the System\n",
    "#  NB Updated 2016 for package changes around scores\n",
    "\n",
    "import re, math, collections, itertools, sys, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures, scores\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "def evaluate_features(feature_select):\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    #Opens the positive,negative files.\n",
    "    negSentences = open('rt-polarity-neg.txt', 'r', encoding='utf8')\n",
    "    posSentences = open('rt-polarity-pos.txt', 'r', encoding='utf8')\n",
    "    \n",
    "    #Split the file content by newline character.\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "    #print(negSentences)\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    #print(posSentences)\n",
    "    \n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    # breaks up the sentences into lists of individual words\n",
    "    # creates instance structures for classifier\n",
    "    \n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "        \n",
    "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    #Runs the classifier on the testFeatures\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "    \n",
    "    #Sets up labels to look at output\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "    for i, (features, label) in enumerate(testFeatures): # enumerate adds number-count to each item\n",
    "        referenceSets[label].add(i)               # recorded polarity for these test sentences\n",
    "        predicted = classifier.classify(features) # classifiers' proposed polarity for tests\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    #Outputs\n",
    "    print('train on %s instances, test on %s instances'% (len(trainFeatures), len(testFeatures)))\n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print('pos precision:', scores.precision(referenceSets['pos'], testSets['pos']))\n",
    "    print('pos recall:', scores.recall(referenceSets['pos'], testSets['pos']))\n",
    "    print('neg precision:', scores.precision(referenceSets['neg'], testSets['neg']))\n",
    "    print('neg recall:', scores.recall(referenceSets['neg'], testSets['neg']))\n",
    "    classifier.show_most_informative_features(10)\n",
    "\n",
    "def make_full_dict(words):\n",
    "    #print('Words:',words)\n",
    "    #print(dict([(word, True) for word in words]))\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print('using all words as features')\n",
    "evaluate_features(make_full_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stopword removal\n",
      "using all words as features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.7625656414103525\n",
      "pos precision: 0.7611940298507462\n",
      "pos recall: 0.7651912978244562\n",
      "neg precision: 0.7639517345399698\n",
      "neg recall: 0.759939984996249\n"
     ]
    }
   ],
   "source": [
    "import re, math, collections, itertools, sys, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures, scores\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "\n",
    "def evaluate_features(feature_select):\n",
    "\n",
    "    negSentences = open('rt-polarity-neg.txt', 'r', encoding='utf8')\n",
    "    posSentences = open('rt-polarity-pos.txt', 'r', encoding='utf8')\n",
    "\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "\n",
    "    tweet_tk = TweetTokenizer() \n",
    "    swords = stopwords.words('english')\n",
    "    tweets_pos = []\n",
    "    tweets_neg = []\n",
    "\n",
    "    for i in posSentences:\n",
    "        tokenised_tweet=tweet_tk.tokenize(i)\n",
    "        lower_tweet = [i.lower() for i in tokenised_tweet]\n",
    "        for l in range(len(lower_tweet)):\n",
    "            lower_tweet[l]= re.sub(r'[\\d]', ' ', lower_tweet[l])\n",
    "            lower_tweet[l]= re.sub(r'[^\\w_]','',lower_tweet[l])\n",
    "        tweets_pos.append([w for w in lower_tweet if w not in swords])\n",
    "\n",
    "    final_pos = []\n",
    "    for tweet in tweets_pos:\n",
    "        tweet_cleaned = [a for a in tweet if a]\n",
    "        \" \".join(tweet_cleaned)\n",
    "        final_pos.append(\" \".join(tweet_cleaned))\n",
    "        #final_pos.append(tweet_cleaned)\n",
    "\n",
    "    #print('Positive:',final_pos[0])\n",
    "    posSentences = final_pos\n",
    "    #print(posSentences[0])\n",
    "\n",
    "    for i in negSentences:\n",
    "        tokenised_tweet=tweet_tk.tokenize(i)\n",
    "        lower_tweet = [i.lower() for i in tokenised_tweet]\n",
    "        for l in range(len(lower_tweet)):\n",
    "            lower_tweet[l]= re.sub(r'[\\d]', ' ', lower_tweet[l])\n",
    "            lower_tweet[l]= re.sub(r'[^\\w_]','',lower_tweet[l])\n",
    "        tweets_neg.append([w for w in lower_tweet if w not in swords])\n",
    "\n",
    "    final_neg = []\n",
    "    for tweet in tweets_neg:\n",
    "        tweet_cleaned = [a for a in tweet if a]\n",
    "        final_neg.append(\" \".join(tweet_cleaned))\n",
    "    #print('\\n\\nNegative:',final_neg[0])\n",
    "    negSentences = final_neg\n",
    "    #print(negSentences[0])\n",
    "    \n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    # breaks up the sentences into lists of individual words\n",
    "    # creates instance structures for classifier\n",
    "    \n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "        \n",
    "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    #Runs the classifier on the testFeatures\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "    \n",
    "    #Sets up labels to look at output\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "    for i, (features, label) in enumerate(testFeatures): # enumerate adds number-count to each item\n",
    "        referenceSets[label].add(i)               # recorded polarity for these test sentences\n",
    "        predicted = classifier.classify(features) # classifiers' proposed polarity for tests\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    #Outputs\n",
    "    print('train on %s instances, test on %s instances'% (len(trainFeatures), len(testFeatures)))\n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print('pos precision:', scores.precision(referenceSets['pos'], testSets['pos']))\n",
    "    print('pos recall:', scores.recall(referenceSets['pos'], testSets['pos']))\n",
    "    print('neg precision:', scores.precision(referenceSets['neg'], testSets['neg']))\n",
    "    print('neg recall:', scores.recall(referenceSets['neg'], testSets['neg']))\n",
    "    #classifier.show_most_informative_features(10)\n",
    "    \n",
    "def make_full_dict(words):\n",
    "    #print('Words:',words)\n",
    "    #print(dict([(word, True) for word in words]))\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print('After Stopword removal')\n",
    "print('using all words as features')\n",
    "evaluate_features(make_full_dict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b Different Classifier: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.7475618904726181\n",
      "pos precision: 0.7511415525114156\n",
      "pos recall: 0.7404351087771943\n",
      "neg precision: 0.7440828402366864\n",
      "neg recall: 0.754688672168042\n"
     ]
    }
   ],
   "source": [
    "# http://andybromberg.com/sentiment-analysis-python/\n",
    "# Andy Bromberg's Simple Sentiment Analysis System\n",
    "# Uses data from Pang & Lee (2005)\n",
    "# Uses a Naive Bayes Classifier Train the System\n",
    "# NB Updated 2016 for package changes around scores\n",
    "\n",
    "import re, math, collections, itertools, sys, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures, scores\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def evaluate_features(feature_select):\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    #Opens the positive,negative files.\n",
    "    negSentences = open('rt-polarity-neg.txt', 'r', encoding='utf8')\n",
    "    posSentences = open('rt-polarity-pos.txt', 'r', encoding='utf8')\n",
    "    \n",
    "    #Split the file content by newline character.\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "    #print(negSentences)\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    #print(posSentences)\n",
    "    \n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    # breaks up the sentences into lists of individual words\n",
    "    # creates instance structures for classifier\n",
    "    \n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "        \n",
    "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    #Runs the classifier on the testFeatures\n",
    "    #classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "    classifier = SklearnClassifier(LinearSVC())\n",
    "    classifier.train(trainFeatures)\n",
    "    \n",
    "    #Sets up labels to look at output\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "    for i, (features, label) in enumerate(testFeatures): # enumerate adds number-count to each item\n",
    "        referenceSets[label].add(i)               # recorded polarity for these test sentences\n",
    "        predicted = classifier.classify(features) # classifiers' proposed polarity for tests\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    #Outputs\n",
    "    print('train on %s instances, test on %s instances'% (len(trainFeatures), len(testFeatures)))\n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print('pos precision:', scores.precision(referenceSets['pos'], testSets['pos']))\n",
    "    print('pos recall:', scores.recall(referenceSets['pos'], testSets['pos']))\n",
    "    print('neg precision:', scores.precision(referenceSets['neg'], testSets['neg']))\n",
    "    print('neg recall:', scores.recall(referenceSets['neg'], testSets['neg']))\n",
    "    #classifier.show_most_informative_features(10)\n",
    "\n",
    "def make_full_dict(words):\n",
    "    #print('Words:',words)\n",
    "    #print(dict([(word, True) for word in words]))\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print('using all words as features')\n",
    "evaluate_features(make_full_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b : Different Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 9062 instances, test on 1600 instances\n",
      "accuracy: 0.781875\n",
      "pos precision: 0.7955439056356488\n",
      "pos recall: 0.75875\n",
      "neg precision: 0.7694145758661888\n",
      "neg recall: 0.805\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     20.3 : 1.0\n",
      "                 generic = True              neg : pos    =     14.3 : 1.0\n",
      "                   flaws = True              pos : neg    =     14.3 : 1.0\n",
      "              refreshing = True              pos : neg    =     13.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "               inventive = True              pos : neg    =     13.0 : 1.0\n",
      "                  boring = True              neg : pos    =     12.7 : 1.0\n",
      "                    flat = True              neg : pos    =     12.6 : 1.0\n",
      "                 routine = True              neg : pos    =     12.3 : 1.0\n",
      "evaluating best 10 word features\n",
      "train on 9062 instances, test on 1600 instances\n",
      "accuracy: 0.5\n",
      "pos precision: 0.5\n",
      "pos recall: 1.0\n",
      "neg precision: None\n",
      "neg recall: 0.0\n",
      "Most Informative Features\n",
      "                       ó = True              pos : neg    =      1.7 : 1.0\n",
      "                       ó = None              neg : pos    =      1.0 : 1.0\n",
      "evaluating best 100 word features\n",
      "train on 9062 instances, test on 1600 instances\n",
      "accuracy: 0.5\n",
      "pos precision: 0.5\n",
      "pos recall: 0.99\n",
      "neg precision: 0.5\n",
      "neg recall: 0.01\n",
      "Most Informative Features\n",
      "                       z = True              pos : neg    =      3.0 : 1.0\n",
      "                   you'd = True              neg : pos    =      2.4 : 1.0\n",
      "                    zeal = True              neg : pos    =      2.3 : 1.0\n",
      "                 younger = True              neg : pos    =      2.2 : 1.0\n",
      "                    zone = True              pos : neg    =      2.1 : 1.0\n",
      "                   zhang = True              pos : neg    =      1.8 : 1.0\n",
      "                youthful = True              pos : neg    =      1.8 : 1.0\n",
      "                       ó = True              pos : neg    =      1.7 : 1.0\n",
      "              youngsters = True              pos : neg    =      1.7 : 1.0\n",
      "                 yorkers = True              neg : pos    =      1.7 : 1.0\n",
      "evaluating best 1000 word features\n",
      "train on 9062 instances, test on 1600 instances\n",
      "accuracy: 0.570625\n",
      "pos precision: 0.5446640316205533\n",
      "pos recall: 0.86125\n",
      "neg precision: 0.6686567164179105\n",
      "neg recall: 0.28\n",
      "Most Informative Features\n",
      "               wonderful = True              pos : neg    =     11.4 : 1.0\n",
      "                    warm = True              pos : neg    =      7.9 : 1.0\n",
      "                   waste = True              neg : pos    =      7.8 : 1.0\n",
      "                  wasted = True              neg : pos    =      7.7 : 1.0\n",
      "                     wry = True              pos : neg    =      7.7 : 1.0\n",
      "                  warmth = True              pos : neg    =      6.6 : 1.0\n",
      "              washington = True              pos : neg    =      6.3 : 1.0\n",
      "                  wasn't = True              neg : pos    =      6.2 : 1.0\n",
      "                   worst = True              neg : pos    =      5.9 : 1.0\n",
      "                 winning = True              pos : neg    =      5.4 : 1.0\n",
      "evaluating best 10000 word features\n",
      "train on 9062 instances, test on 1600 instances\n",
      "accuracy: 0.745\n",
      "pos precision: 0.7692307692307693\n",
      "pos recall: 0.7\n",
      "neg precision: 0.7247706422018348\n",
      "neg recall: 0.79\n",
      "Most Informative Features\n",
      "              refreshing = True              pos : neg    =     13.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "                 routine = True              neg : pos    =     12.3 : 1.0\n",
      "             mesmerizing = True              pos : neg    =     11.7 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "               wonderful = True              pos : neg    =     11.4 : 1.0\n",
      "                  stupid = True              neg : pos    =     11.0 : 1.0\n",
      "                mindless = True              neg : pos    =     11.0 : 1.0\n",
      "                provides = True              pos : neg    =     10.6 : 1.0\n",
      "                   stale = True              neg : pos    =     10.3 : 1.0\n",
      "evaluating best 15000 word features\n",
      "train on 9062 instances, test on 1600 instances\n",
      "accuracy: 0.771875\n",
      "pos precision: 0.7888446215139442\n",
      "pos recall: 0.7425\n",
      "neg precision: 0.7567886658795749\n",
      "neg recall: 0.80125\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     20.3 : 1.0\n",
      "                 generic = True              neg : pos    =     14.3 : 1.0\n",
      "                   flaws = True              pos : neg    =     14.3 : 1.0\n",
      "              refreshing = True              pos : neg    =     13.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               inventive = True              pos : neg    =     13.0 : 1.0\n",
      "                    flat = True              neg : pos    =     12.6 : 1.0\n",
      "                 routine = True              neg : pos    =     12.3 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "             mesmerizing = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import re, math, collections, itertools, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk import(precision, recall)\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "POLARITY_DATA_DIR = os.path.join('polarityData', 'rt-polaritydata')\n",
    "RT_POLARITY_POS_FILE = os.path.join('rt-polarity-pos.txt')\n",
    "RT_POLARITY_NEG_FILE = os.path.join('rt-polarity-neg.txt')\n",
    "\n",
    "\n",
    "#this function takes a feature selection mechanism and returns its performance in a variety of metrics\n",
    "def evaluate_features(feature_select):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    #http://stackoverflow.com/questions/367155/splitting-a-string-into-words-and-punctuation\n",
    "    #breaks up the sentences into lists of individual words (as selected by the input mechanism) and appends 'pos' or 'neg' after each list\n",
    "    with open(RT_POLARITY_POS_FILE, 'r',encoding = 'utf-8') as posSentences:\n",
    "        for i in posSentences:\n",
    "            posWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "            #posWords = [word for word in posWords if not word in stop]\n",
    "            posWords = [feature_select(posWords), 'pos']\n",
    "            posFeatures.append(posWords)\n",
    "                    \n",
    "    with open(RT_POLARITY_NEG_FILE, 'r',encoding = 'utf-8') as negSentences:\n",
    "        for i in negSentences:\n",
    "            negWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "            #negWords = [word for word in negWords if not word in stop]\n",
    "            negWords = [feature_select(negWords), 'neg']\n",
    "            negFeatures.append(negWords)\n",
    "\n",
    "\n",
    "    #selects 3/4 of the features to be used for training and 1/4 to be used for testing\n",
    "    posCutoff = int(math.floor(len(posFeatures)*0.85))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*0.85))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "\n",
    "    #trains a Naive Bayes Classifier\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\t\n",
    "\n",
    "    #initiates referenceSets and testSets\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\t\n",
    "\n",
    "    #puts correctly labeled sentences in referenceSets and the predictively labeled version in testsets\n",
    "    for i, (features, label) in enumerate(testFeatures):\n",
    "        referenceSets[label].add(i)\n",
    "        predicted = classifier.classify(features)\n",
    "        testSets[predicted].add(i)\t\n",
    "\n",
    "    #prints metrics to show how well the feature selection did\n",
    "    print ('train on %d instances, test on %d instances' % (len(trainFeatures), len(testFeatures)))\n",
    "    print ('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print ('pos precision:', precision(referenceSets['pos'], testSets['pos']))\n",
    "    print ('pos recall:', recall(referenceSets['pos'], testSets['pos']))\n",
    "    print ('neg precision:', precision(referenceSets['neg'], testSets['neg']))\n",
    "    print ('neg recall:', recall(referenceSets['neg'], testSets['neg']))\n",
    "    classifier.show_most_informative_features(10)\n",
    "\n",
    "#creates a feature selection mechanism that uses all words\n",
    "def make_full_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "#tries using all words as the feature selection mechanism\n",
    "print ('using all words as features')\n",
    "evaluate_features(make_full_dict)\n",
    "\n",
    "#scores words based on chi-squared test to show information gain (http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/)\n",
    "def create_word_scores():\n",
    "    #creates lists of all positive and negative words\n",
    "    posWords = []\n",
    "    negWords = []\n",
    "    with open(RT_POLARITY_POS_FILE, 'r',encoding = 'utf-8') as posSentences:\n",
    "        for i in posSentences:\n",
    "            posWord = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "            posWords.append(posWord)\n",
    "    with open(RT_POLARITY_NEG_FILE, 'r',encoding = 'utf-8') as negSentences:\n",
    "        for i in negSentences:\n",
    "            negWord = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "            negWords.append(negWord)\n",
    "    posWords = list(itertools.chain(*posWords))\n",
    "    negWords = list(itertools.chain(*negWords))\n",
    "\n",
    "    #build frequency distibution of all words and then frequency distributions of words within positive and negative labels\n",
    "    word_fd = FreqDist()\n",
    "    cond_word_fd = ConditionalFreqDist()\n",
    "    for word in posWords:\n",
    "        word_fd[word.lower()] += 1\n",
    "        cond_word_fd['pos'][word.lower()] += 1\n",
    "    for word in negWords:\n",
    "        word_fd[word.lower()] += 1\n",
    "        cond_word_fd['neg'][word.lower()] += 1\n",
    "\n",
    "    #finds the number of positive and negative words, as well as the total number of words\n",
    "    pos_word_count = cond_word_fd['pos'].N()\n",
    "    neg_word_count = cond_word_fd['neg'].N()\n",
    "    total_word_count = pos_word_count + neg_word_count\n",
    "\n",
    "    #builds dictionary of word scores based on chi-squared test\n",
    "    word_scores = {}\n",
    "    for word, freq in word_fd.items():\n",
    "        pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word], (freq, pos_word_count), total_word_count)\n",
    "        neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word], (freq, neg_word_count), total_word_count)\n",
    "        word_scores[word] = pos_score + neg_score\n",
    "\n",
    "    return word_scores\n",
    "\n",
    "#finds word scores\n",
    "word_scores = create_word_scores()\n",
    "\n",
    "#finds the best 'number' words based on word scores\n",
    "def find_best_words(word_scores, number):\n",
    "    best_vals = sorted(word_scores.items(), key=lambda s: s, reverse=True)[:number]\n",
    "    best_words = set([w for w, s in best_vals])\n",
    "    return best_words\n",
    "\n",
    "#creates feature selection mechanism that only uses best words\n",
    "def best_word_features(words):\n",
    "    return dict([(word, True) for word in words if word in best_words])\n",
    "\n",
    "#numbers of features to select\n",
    "numbers_to_test = [10, 100, 1000, 10000, 15000]\n",
    "#tries the best_word_features mechanism with each of the numbers_to_test of features\n",
    "for num in numbers_to_test:\n",
    "    print ('evaluating best %d word features' % (num))\n",
    "    best_words = find_best_words(word_scores, num)\n",
    "    evaluate_features(best_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
